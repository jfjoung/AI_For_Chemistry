{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyx8/9e7zFXS8M+xbinc1j"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfjoung/AI_For_Chemistry/blob/main/Week_3_Introduction_to_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ],
      "metadata": {
        "id": "X681RBhxf1XX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎯 **학습 목표:**  \n",
        "- **딥러닝의 기본 개념**과 **뉴럴 네트워크의 주요 구성 요소**를 이해한다.  \n",
        "- **기본 뉴럴 네트워크 모델을 설계, 훈련 및 평가하는 과정**을 학습한다.  \n",
        "- **딥러닝 프레임워크의 사용 방법**을 익히고, 간단한 뉴럴 네트워크를 훈련시켜본다.  \n",
        "- **딥러닝 모델의 주요 하이퍼파라미터**(예: 학습률, 에포크 수)를 이해하고, 모델 성능을 개선하기 위해 조정해본다.  \n",
        "- **딥러닝 워크플로우**의 전반적인 이해를 통해, 새로운 문제에 대한 접근 방법을 배운다.  \n"
      ],
      "metadata": {
        "id": "gc_Dw3w2hNe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. 관련 패키지\n",
        "\n",
        "### PyTorch\n",
        "Torch 라이브러리를 기반으로 한 PyTorch는 기계 학습 실무자들 사이에서 가장 인기 있는 딥러닝 프레임워크 중 하나입니다. PyTorch를 사용하여 딥러닝 작업을 수행하는 방법을 학습할 예정입니다. 추가 자료는 PyTorch의 [튜토리얼](https://pytorch.org/tutorials/)과 [문서](https://pytorch.org/docs/stable/index.html)를 참고하세요.\n",
        "\n",
        "### PyTorch Lightning\n",
        "PyTorch Lightning은 AI 연구자 및 기계 학습 엔지니어를 위한 딥러닝 프레임워크로, 대규모 환경에서도 성능을 포기하지 않고 최대한의 유연성을 제공합니다. 추가 자료는 PyTorch Lightning의 [문서](https://pytorch-lightning.readthedocs.io/en/stable/)를 참고하세요.\n"
      ],
      "metadata": {
        "id": "PIxNiOM2hf4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6zdANjTfnDO"
      },
      "outputs": [],
      "source": [
        "# 필수 라이브러리 설치\n",
        "! pip install pytorch-lightning wandb rdkit ogb deepchem torch\n",
        "\n",
        "# 데이터 다운로드\n",
        "! mkdir data/  # 데이터를 저장할 디렉토리 생성\n",
        "# ESOL 데이터셋 다운로드\n",
        "! wget https://raw.githubusercontent.com/jfjoung/AI_For_Chemistry/main/data/week3/esol.csv -O data/esol.csv\n",
        "# ESOL 유틸리티 코드 다운로드\n",
        "! wget https://raw.githubusercontent.com/jfjoung/AI_For_Chemistry/main/data/week3/esol_utils.py -O esol_utils.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실험의 재현성을 보장하기 위해 난수 시드를 설정합니다.\n"
      ],
      "metadata": {
        "id": "D15HumUii2vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 실험의 재현성을 보장하기 위해 난수 시드를 설정합니다.\n",
        "torch.manual_seed(0)  # PyTorch의 난수 시드 설정\n",
        "torch.cuda.manual_seed(0)  # CUDA 연산 시 일관된 결과를 보장하기 위한 난수 시드 설정\n",
        "np.random.seed(0)  # NumPy의 난수 시드 설정\n",
        "random.seed(0)  # Python 기본 random 모듈의 난수 시드 설정\n"
      ],
      "metadata": {
        "id": "fAwM1-gAh72h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 지도 학습 기반 딥러닝\n",
        "\n",
        "지난 수업에서 지도 학습(supervised learning)에 대해 익숙해졌을 것입니다. 지도 학습은 레이블이 있는 데이터셋을 사용하여 입력과 출력 간의 관계를 학습하는 머신 러닝의 한 유형입니다.\n",
        "\n",
        "지금까지 살펴본 모델들은 비교적 간단하며 특정 상황에서는 잘 작동하지만, 때때로 충분하지 않을 수도 있습니다. 이런 경우에는 어떻게 해야 할까요?\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/jfjoung/AI_For_Chemistry/main/img/deeper_meme.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "### 딥러닝\n",
        "딥러닝(deep learning)은 인공 신경망을 학습시켜 데이터로부터 학습하는 머신 러닝의 하위 분야입니다. 전통적인 머신 러닝 알고리즘은 주로 수작업으로 설계된 특징(feature)과 선형 모델에 의존하는 반면, 딥러닝 알고리즘은 원 데이터(raw data)에서 자동으로 특징을 학습하고 계층적 표현(hierarchy of representations)을 학습할 수 있습니다. 이러한 특성 덕분에 딥러닝 모델은 분자 특성 예측, 반응 예측, 역합성(retrosynthesis) 등과 같은 화학 분야의 다양한 작업에서 최첨단 성능을 달성할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "4vfZzoJtjC-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터: [ESOL 데이터셋](https://pubs.acs.org/doi/10.1021/ci034243x)으로 돌아가기\n",
        "지난주에 사용했던 ESOL 데이터셋을 다시 활용하겠습니다.  \n",
        "이전 모델들과의 결과를 비교할 수 있도록 동일한 데이터를 사용합니다.  \n",
        "데이터 로딩과 전처리를 위해 지난주 코드도 그대로 재사용할 것입니다.\n"
      ],
      "metadata": {
        "id": "9Z6IhjDJjuMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from esol_utils import load_esol_data\n",
        "\n",
        "# ESOL 데이터셋을 로드하여 학습, 검증, 테스트 세트로 분할합니다.\n",
        "(X_train, X_valid, X_test, y_train, y_valid, y_test, scaler) = load_esol_data()\n"
      ],
      "metadata": {
        "id": "ZnViE2zQi7nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 신경망 (Neural Networks)\n",
        "\n",
        "신경망은 인간의 뇌가 작동하는 방식을 모방하도록 설계된 머신 러닝 모델의 한 유형입니다.\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/jfjoung/AI_For_Chemistry/main/img/nn_image.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "\\\n",
        "신경망은 여러 개의 계층(layer)으로 구성된 노드(node)들로 이루어져 있으며, 각 노드는 입력값에 `선형 함수(linear function)`를 적용합니다.  \n",
        "또한, 비선형 활성 함수(non-linear activation function)를 사용하여 모델에 `비선형성(non-linearity)`을 도입함으로써, 보다 복잡한 패턴을 학습할 수 있도록 합니다.\n"
      ],
      "metadata": {
        "id": "OZuTbu85lpsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 필요한 라이브러리 및 모듈을 불러옵니다.\n",
        "# - os: 운영 체제와 상호 작용하기 위한 모듈\n",
        "# - torch: PyTorch 라이브러리\n",
        "# - nn: 신경망 모델을 구성하는 PyTorch의 뉴럴 네트워크 모듈\n",
        "# - F: PyTorch의 함수형 API\n",
        "# - pytorch_lightning: PyTorch의 고수준 인터페이스로 모델 학습을 간소화\n",
        "# - DataLoader: PyTorch에서 데이터 배치를 로드하는 유틸리티\n"
      ],
      "metadata": {
        "id": "W_VE9B2uj3LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 딥러닝 모델 만들기\n",
        "\n",
        "요즘에는 딥러닝(DL) 모델을 만드는 것이 비교적 쉬워졌습니다. 특히, [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/index.html)과 같은 라이브러리 덕분에 많은 작업이 자동화되었지만, 여전히 모델에 대한 세부적인 제어가 가능합니다.\n",
        "\n",
        "PyTorch Lightning을 사용하려면 먼저 **클래스(class)**에 대해 알아야 합니다.\n",
        "\n",
        "> 클래스는 특정 속성과 동작을 가진 객체를 생성하기 위한 템플릿 또는 지침 세트라고 생각할 수 있습니다.  \n",
        "> 클래스에서 생성된 객체를 인스턴스(instance)라고 합니다.\n",
        "\n",
        "\\\n",
        "예를 들어, 개(Dog)를 표현하는 프로그램을 만든다고 가정해봅시다.\n",
        "\n",
        "```python\n",
        "class Dog:\n",
        "    def __init__(self, name, color):\n",
        "        self.name = name\n",
        "        self.color = color\n",
        "        \n",
        "    def say_your_name(self):\n",
        "        print(f\"내 이름은 {self.name}입니다.\")\n",
        "```\n",
        "\n",
        "이 예제에서 개는 `name`과 `color`라는 두 개의 속성을 가집니다. 또한, `say_your_name`이라는 메서드 (method)를 가지고 있습니다.\n",
        "\n",
        "이제 원하는 만큼 개 객체를 만들 수 있습니다.\n",
        "\n",
        "```python\n",
        "lassie = Dog(name=\"Lassie\", color=\"White\")\n",
        "pluto = Dog(name=\"Pluto\", color=\"Yellow\")\n",
        "```\n",
        "\n",
        "그리고 객체의 메서드를 호출하여 사용할 수 있습니다.\n",
        "```python\n",
        "pluto.say_your_name()   # 출력: \"내 이름은 Pluto입니다.\"\n",
        "```\n",
        "\n",
        "<font color=\"#4caf50\" size=4> 이제 신경망(Neural Network) 클래스를 정의해봅시다. </font>\n",
        "\n",
        "각 부분의 역할:\n",
        "\n",
        "- **`__init__`**: 모델의 아키텍처를 정의하는 부분입니다.  \n",
        "  다양한 레이어(layer, 모델의 구성 요소)를 추가할 수 있으며, 이곳에서 모델 구조를 결정합니다.\n",
        "\n",
        "- **`training_step`**: 모델 학습 과정에서 사용되는 메서드로, 옵티마이저를 이용해 모델의 파라미터를 업데이트합니다.\n",
        "\n",
        "- **`configure_optimizers`**: 옵티마이저를 설정하는 메서드입니다.  \n",
        "  여기에서 학습률(learning rate)과 사용할 옵티마이저를 정의합니다.\n",
        "\n",
        "- **`forward`**: 입력값이 주어졌을 때 모델이 수행해야 할 연산을 지정합니다.\n"
      ],
      "metadata": {
        "id": "S3i-1m8kmNQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class NeuralNetwork(pl.LightningModule):\n",
        "    def __init__(self, input_sz, hidden_sz, train_data, valid_data, test_data, batch_size=254, lr=1e-3):\n",
        "        super().__init__()  # 부모 클래스(pl.LightningModule)의 초기화 메서드 호출\n",
        "        self.lr = lr  # 학습률(learning rate) 설정\n",
        "        self.train_data = train_data  # 학습 데이터 저장\n",
        "        self.valid_data = valid_data  # 검증 데이터 저장\n",
        "        self.test_data = test_data  # 테스트 데이터 저장\n",
        "        self.batch_size = batch_size  # 배치 크기 설정\n",
        "\n",
        "        # 신경망 모델 구성 요소 정의\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_sz, hidden_sz),  # 입력 크기(input_sz)에서 은닉층(hidden_sz)으로 변환\n",
        "            nn.ReLU(),  # 활성화 함수 ReLU 적용\n",
        "            nn.Linear(hidden_sz, hidden_sz),  # 두 번째 은닉층 추가\n",
        "            nn.ReLU(),  # 활성화 함수 ReLU 적용\n",
        "            nn.Linear(hidden_sz, 1)  # 마지막 출력층 (예측값은 하나의 연속적인 값)\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # 모델 학습 단계 정의\n",
        "        x, y = batch  # 배치 데이터에서 입력(x)과 정답(y) 분리\n",
        "        z = self.model(x)  # 모델을 이용해 예측값 계산\n",
        "        loss = F.mse_loss(z, y)  # 평균 제곱 오차(MSE) 손실 계산\n",
        "        self.log(\"Train loss\", loss)  # 학습 손실 로깅\n",
        "        return loss  # 손실 반환\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # 검증 단계 정의 (에포크가 끝날 때 실행됨)\n",
        "        x, y = batch  # 배치 데이터에서 입력(x)과 정답(y) 분리\n",
        "        z = self.model(x)  # 모델을 이용해 예측값 계산\n",
        "        loss = F.mse_loss(z, y)  # 평균 제곱 오차(MSE) 손실 계산\n",
        "        self.log(\"Valid MSE\", loss)  # 검증 손실 로깅\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # 테스트 단계 정의 (테스트 데이터셋 평가)\n",
        "        x, y = batch  # 배치 데이터에서 입력(x)과 정답(y) 분리\n",
        "        z = self.model(x)  # 모델을 이용해 예측값 계산\n",
        "        loss = F.mse_loss(z, y)  # 평균 제곱 오차(MSE) 손실 계산\n",
        "        self.log(\"Test MSE\", loss)  # 테스트 손실 로깅\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # 옵티마이저(최적화 알고리즘) 설정\n",
        "        optimizer = torch.optim.Adam(  # Adam 옵티마이저 사용\n",
        "            self.parameters(),  # 모델의 학습 가능한 모든 파라미터 전달\n",
        "            lr=self.lr  # 학습률 설정\n",
        "        )\n",
        "        return optimizer  # 옵티마이저 반환\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 모델의 순전파(forward pass) 정의\n",
        "        return self.model(x).flatten()  # 신경망을 통과한 결과를 1차원으로 변환하여 반환\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # 학습 데이터 로더 생성\n",
        "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)  # 배치를 섞어서 학습\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # 검증 데이터 로더 생성\n",
        "        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)  # 배치 순서 유지\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # 테스트 데이터 로더 생성\n",
        "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)  # 배치 순서 유지\n"
      ],
      "metadata": {
        "id": "qoUc3XVomE56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋(Dataset) 클래스\n",
        "\n",
        "Lightning을 사용하려면 `Dataset` 클래스를 생성해야 합니다.  \n",
        "조금 복잡해 보일 수 있지만, 더 복잡한 상황에서도 유연성을 제공하므로 겁먹지 않아도 됩니다! 😉\n"
      ],
      "metadata": {
        "id": "iBlK6tdGm-ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset  # PyTorch의 Dataset 클래스를 가져옴\n",
        "\n",
        "class ESOLDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        ESOL 데이터셋을 PyTorch Dataset 형태로 변환하는 클래스\n",
        "        :param X: 입력 데이터 (특징 벡터)\n",
        "        :param y: 타겟 데이터 (정답 라벨)\n",
        "        \"\"\"\n",
        "        self.X = X  # 입력 데이터 저장\n",
        "        self.y = y  # 타겟 데이터 저장\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        데이터셋의 샘플 개수를 반환\n",
        "        :return: 데이터셋의 전체 크기\n",
        "        \"\"\"\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        인덱스를 기반으로 데이터 샘플을 가져오는 메서드\n",
        "        :param idx: 샘플의 인덱스\n",
        "        :return: 변환된 입력 데이터(X_), 타겟 데이터(y_)\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):  # 인덱스가 텐서 형태일 경우 리스트로 변환\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        X_ = torch.as_tensor(self.X[idx].astype(np.float32))  # 입력 데이터를 PyTorch 텐서로 변환\n",
        "        y_ = torch.as_tensor(self.y[idx].astype(np.float32).reshape(-1))  # 타겟 데이터를 PyTorch 텐서로 변환 및 차원 조정\n",
        "\n",
        "        return X_, y_  # 변환된 데이터를 반환\n",
        "\n",
        "# 데이터셋 인스턴스 생성\n",
        "train_data = ESOLDataset(X_train, y_train)  # 학습 데이터\n",
        "valid_data = ESOLDataset(X_valid, y_valid)  # 검증 데이터\n",
        "test_data = ESOLDataset(X_test, y_test)  # 테스트 데이터\n"
      ],
      "metadata": {
        "id": "CZho5rgum6H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 모델 인스턴스 생성\n",
        "# 하이퍼파라미터를 변경하며 실험해 볼 수 있음\n",
        "nn_model = NeuralNetwork(\n",
        "    input_sz=X_train.shape[1],  # 입력 크기 설정\n",
        "    hidden_sz=128,  # 은닉층 크기 설정 (임의의 값, 필요에 따라 변경 가능)\n",
        "    train_data=train_data,  # 학습 데이터\n",
        "    valid_data=valid_data,  # 검증 데이터\n",
        "    test_data=test_data,  # 테스트 데이터\n",
        "    lr=1e-3,  # 학습률 설정 (임의의 값, 필요에 따라 변경 가능)\n",
        "    batch_size=256  # 배치 크기 설정 (임의의 값, 필요에 따라 변경 가능)\n",
        ")\n",
        "\n",
        "# PyTorch Lightning Trainer 설정 (모델 학습 방식 지정)\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=50,  # 최대 학습 에포크 수 설정 (임의의 값, 필요에 따라 변경 가능)\n",
        ")\n",
        "\n",
        "# 모델 학습 시작\n",
        "trainer.fit(model=nn_model)\n",
        "\n",
        "# 테스트 실행\n",
        "results = trainer.test(ckpt_path=\"best\")\n"
      ],
      "metadata": {
        "id": "L56e_YpnnE8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 RMSE 계산\n",
        "test_mse = results[0][\"Test MSE\"]  # 테스트 데이터에서의 MSE 값 가져오기\n",
        "test_rmse = test_mse ** 0.5  # RMSE(Root Mean Squared Error) 계산\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"\\nANN model performance: RMSE on test set = {test_rmse:.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "Je5GcSlGnOzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 연습 문제:\n",
        "\n",
        "하이퍼파라미터를 조정해 보면서 결과가 어떻게 변하는지 확인해보세요.\n",
        "\n",
        "`hidden_sz`, `batch_sz`, `max_epochs`, `lr` 등의 값을 변경해 볼 수 있습니다.  \n",
        "또는 신경망의 구조 자체를 수정하여, 레이어 개수를 변경하거나 활성화 함수를 바꿔보는 것도 좋은 방법입니다.\n"
      ],
      "metadata": {
        "id": "w_WYKp5MnoeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터를 바꾸어가며 만든 모델 들 중, 무엇이 가장 좋습니까?\n",
        "가장 좋은 이유는 무엇일까요?\n",
        "\n",
        "Test set에 대하여 RMSE가 가장 작은 모델로 여러 분자들의 용해도를 예측해봅시다."
      ],
      "metadata": {
        "id": "cWURSkirpLXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepchem.feat import RDKitDescriptors\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "\n",
        "def smiles_to_descriptors(smiles, scaler):\n",
        "    \"\"\"\n",
        "    SMILES 문자열을 RDKit Descriptors로 변환하는 함수\n",
        "    :param smiles: 변환할 SMILES 문자열\n",
        "    :param scaler: 학습 데이터에서 학습된 MinMaxScaler 객체\n",
        "    :return: 변환된 특징 벡터 (PyTorch 텐서)\n",
        "    \"\"\"\n",
        "    featurizer = RDKitDescriptors()  # RDKit 분자 기술자(Descriptors) 생성\n",
        "    features = featurizer.featurize([smiles])  # 단일 SMILES를 특징 벡터로 변환\n",
        "    features = np.array(features, dtype=np.float32)  # NumPy 배열 변환\n",
        "\n",
        "    # 학습 데이터와 일관성을 유지하기 위해 동일한 스케일링 적용\n",
        "    features = features[:, ~np.isnan(features).any(axis=0)]  # NaN 값 제거\n",
        "    features = scaler.transform(features)  # 학습된 스케일러를 적용하여 변환\n",
        "    return torch.tensor(features).float()  # PyTorch 텐서로 변환\n",
        "\n",
        "def predict_solubility(model, smiles, scaler):\n",
        "    \"\"\"\n",
        "    학습된 모델을 사용하여 SMILES의 용해도를 예측하는 함수\n",
        "    :param model: 학습된 신경망 모델\n",
        "    :param smiles: 용해도를 예측할 SMILES 문자열\n",
        "    :param scaler: 데이터 스케일링을 위한 MinMaxScaler 객체\n",
        "    :return: 예측된 용해도 값\n",
        "    \"\"\"\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        X = smiles_to_descriptors(smiles, scaler)  # SMILES를 특징 벡터로 변환\n",
        "        prediction = model(X).item()  # 모델을 통해 예측 수행\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def calculate_molecular_weight(smiles):\n",
        "    \"\"\"\n",
        "    주어진 SMILES 문자열의 1 mol당 g(분자량, Molecular Weight)를 계산하는 함수\n",
        "    :param smiles: 분자 구조를 나타내는 SMILES 문자열\n",
        "    :return: 분자량 (g/mol)\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)  # SMILES 문자열을 RDKit 분자 객체로 변환\n",
        "    if mol is None:\n",
        "        raise ValueError(\"유효하지 않은 SMILES 문자열입니다.\")\n",
        "\n",
        "    mw = Descriptors.MolWt(mol)  # RDKit을 사용하여 분자량 계산\n",
        "    return mw"
      ],
      "metadata": {
        "id": "2gFGeXKdoJ2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = nn_model  # 학습된 모델 사용\n",
        "test_smiles = \"O=C(C)Oc1ccccc1C(=O)O\"  # 예측할 SMILES (예: 아스피린)\n",
        "predicted_log_solubility = predict_solubility(trained_model, test_smiles, scaler)\n",
        "predicted_solubility_mol_L = 10 ** predicted_log_solubility\n",
        "\n",
        "print(f\"SMILES: {test_smiles}\")\n",
        "print(f\"Predicted log Solubility: {predicted_log_solubility:.4f} log(mol/L)\")\n",
        "print(f\"Predicted Solubility: {predicted_solubility_mol_L:.6f} mol/L\")\n",
        "print(f\"Predicted Solubility: {predicted_solubility_mol_L*calculate_molecular_weight(test_smiles):.6f} g/L\")\n",
        "\n",
        "# 아스피린의 용해도는 3 g/L 입니다."
      ],
      "metadata": {
        "id": "8R0TCZeLoKKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzifqsamoQgi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}